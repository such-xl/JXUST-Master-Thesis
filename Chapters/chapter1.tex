\section{引言}

\subsection{研究背景}

现代制造业生产环境日益复杂，具有高柔性、高动态性的特点。动态柔性作业车间调度问题 (DFJSP) 是其中最具挑战性的优化范式之一。调度通过对生产资源的合理安排,以缩短生产时间、提高资源利用率、降低生  产成本,在生产系统中作用显著。 DFJSP 不仅需要决定工序的机床选择（路由）和加工顺序（排序），还必须实时应对一系列动态事件，如随机工件到达、紧急插单、机床故障以及加工时间不确定性等。这些不确定性使得生产环境处于随机性和实时性的持续变化中。
传统的调度方法，包括启发式规则（如 FIFO、SPT、EDD 等）和元启发式算法（如遗传算法、粒子群优化等），主要依赖于离线优化。它们在面对 DFJSP 的复杂和动态环境时，表现出适应性差、计算开销大和难以实时决策的局限性 \cite{liu2023deep,zhao2021iterated}。

为克服这些限制，以深度强化学习 (DRL) 为代表的数据驱动框架应运而生。DRL 结合了深度学习强大的感知能力和强化学习的自适应决策能力，为车间调度提供了实时、高质量的解决方案。在 DFJSP 中，由于存在多台机器和多个工件需要协调，多智能体强化学习 (MARL) 通过结合 RL 的自适应决策与多智能体系统 (MAS) 的分散式协调，被认为是一种特别适合解决这一复杂、高维、非线性问题的强大工具。

\subsection{多智能体强化学习在 DFJSP 中的局限}

虽然多智能体强化学习（MARL）为动态柔性作业车间调度问题（DFJSP）提供了新的研究思路，但其在现有研究中的应用仍面临若干关键挑战。这些挑战主要源于 DFJSP 的复杂性以及现有 MARL 框架在异构系统中固有的局限性。

在先前关于 DFJSP 的 MARL 研究中，调度问题通常被建模为异构智能体系统，即由具有不同角色的智能体（例如负责路由决策的工件智能体和负责排序决策的机器智能体）组成 \cite{zhang2024dynamic, kaven2024multi, pu2024multi, jing2024multi}。

非稳态性：这些智能体存在不对称的观测空间、异构动作空间（路由 vs. 排序）以及事件驱动、异步的决策机制，其动作集随着调度状态动态变化。由于智能体策略的更新会直接改变其他智能体的决策环境，这种交互导致系统呈现强烈的非稳态性，使得学习动力学不稳定 \cite{son2019qtran,jing2024multi}。尽管现有工作广泛采用集中式训练-分散式执行（CTDE） \cite{liu2023deep}、交替训练 \cite{liu2022deep, gergely2024multi} 等方法缓解非稳态问题，但策略耦合和协调开销仍然不可避免，异构系统中的非稳态问题并未得到根本解决。

稀疏奖励：在 DFJSP 中，关键性能指标（如作业完成时间、总 tardiness）通常只能在作业完成或整个调度过程结束时才能获得评估。这导致智能体在训练早期难以获得有效反馈，学习信号稀疏，从而显著降低策略探索效率，并增加训练过程中的不确定性。

收敛困难：非稳态环境和稀疏奖励的双重影响，使得 MARL 系统在 DFJSP 中的收敛速度往往较慢。尤其在大规模或高度动态的车间环境下，智能体之间的协调和策略优化难度显著增加，容易陷入局部最优或产生震荡学习行为，进一步加剧了收敛困难。

综上所述，现有 MARL 方法在处理 DFJSP 时仍存在多智能体交互引发的非稳态性、奖励稀疏以及收敛困难等核心问题。这些挑战表明，需要设计针对智能体间交互的协调机制、改进奖励信号以及加速策略收敛的创新方法，从而为后续提出的单链决策、在线专家引导及内在奖励机制提供研究动机



\subsection{本文研究内容与创新点}
本文针对动态柔性作业车间调度问题（DFJSP）中多智能体强化学习（MARL）面临的非稳态、奖励稀疏与收敛困难问题，提出了一系列研究内容和创新方法，主要包括以下三个方面：

（1）机器对齐的抽象化 MARL 建模框架
本文通过优先级派工规则（Priority Dispatching Rules, PDRs）构建抽象化的状态—动作表示，将不同类型机器的决策逻辑在逻辑上统一，实现单一策略对多智能体的协调控制。该方法不仅简化了异构智能体的建模复杂性，也有效缓解了多智能体交互引发的环境非稳态问题，为高效、可扩展的调度策略学习提供基础。

（2）融入模仿学习的专家引导探索策略
为加速策略收敛并提升训练稳定性，本文引入在线专家机制，通过实时生成或提供专家示例对智能体策略进行引导，使智能体在训练早期便能够形成优良的决策倾向。该策略能够在复杂、多智能体交互的环境中有效减少盲目探索，提高策略学习效率。

（3）融入内在激励机制以提升稀疏奖励下的探索能力
针对 DFJSP 中奖励稀疏或延迟反馈的问题，本文设计了基于好奇心的内在奖励机制，鼓励智能体主动探索潜在关键调度状态。这一机制不仅增强了智能体在稀疏奖励环境下的探索能力，也显著提高了策略的泛化能力与适应性，为动态调度环境中的稳健决策提供支持

\subsection{论文结构安排}


